{
  "questions": [
    {
      "id": "1",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "CIS Benchmark – Kubelet\n\nApply CIS benchmark settings for kubelet:\n\n- In kubelet config: set anonymous auth to false (in config file), use Webhook for authorization mode.\n- In etcd manifest (`/etc/kubernetes/manifests/etcd.yaml`):\n  - Add client certificate authentication flags\n  - Remove insecure listen address flag",
      "concepts": ["CIS benchmark", "kubelet", "etcd", "security"],
      "verification": [
        {
          "id": "1",
          "description": "Kubelet and etcd configuration applied",
          "verificationScriptFile": "q1_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "2",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "Remove Anonymous Access to API Server\n\nConfigure the API server to disable anonymous authentication and enable NodeRestriction admission plugin.\n\nThen remove the ClusterRoleBinding named `anonymous-access` that grants anonymous access to the cluster.",
      "concepts": ["apiserver", "RBAC", "anonymous-auth", "NodeRestriction"],
      "verification": [
        {
          "id": "1",
          "description": "API server and ClusterRoleBinding updated",
          "verificationScriptFile": "q2_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "3",
      "namespace": "team-white",
      "machineHostname": "ckad9999",
      "question": "Image Policy Webhook\n\nEnable ImagePolicyWebhook in apiserver and use config file:\n\n1. Update config file (it's in JSON format) to implicit deny by setting parameter `defaultAllow` to `false` instead of `true`\n2. Update kubeconfig file to point to webhook server URL shared in the question\n3. Update kube-apiserver manifest to enable image policy plugin and specify the admission control config file path",
      "concepts": ["ImagePolicyWebhook", "admission control", "apiserver"],
      "verification": [
        {
          "id": "1",
          "description": "ImagePolicyWebhook configured",
          "verificationScriptFile": "q3_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "4",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "Deployment – readOnlyRootFilesystem\n\nUpdate deployments to be `readOnlyRootFilesystem` to `true` to be immutable.",
      "concepts": ["deployments", "securityContext", "readOnlyRootFilesystem"],
      "verification": [
        {
          "id": "1",
          "description": "Deployments have readOnlyRootFilesystem true",
          "verificationScriptFile": "q4_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "5",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "Deployment – Security Context\n\nUpdate deployment that has 2 containers with security context in each pod:\n- `runAsUser: 30000`\n- `allowPrivilegeEscalation: false`\n- `readOnlyRootFilesystem: true`",
      "concepts": ["deployments", "securityContext", "runAsUser", "allowPrivilegeEscalation"],
      "verification": [
        {
          "id": "1",
          "description": "Deployment has required security context",
          "verificationScriptFile": "q5_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "6",
      "namespace": "team-sedum",
      "machineHostname": "ckad9999",
      "question": "Istio Security and mTLS\n\nIstio has been installed in the cluster. Enable Istio sidecar injection for the whole Namespace `team-sedum` and ensure all current and future Pods are running with the Istio proxy sidecar.",
      "concepts": ["Istio", "sidecar injection", "mTLS"],
      "verification": [
        {
          "id": "1",
          "description": "Namespace has istio-injection and pods have sidecar",
          "verificationScriptFile": "q6_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "7",
      "namespace": "development",
      "machineHostname": "ckad9999",
      "question": "NetworkPolicy – Deny All Traffic\n\nCreate network policy that denies all egress and ingress traffic to pods in `development` namespace.\n\n```\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all-traffic\n  namespace: development\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n```",
      "concepts": ["NetworkPolicy", "ingress", "egress", "deny-all"],
      "verification": [
        {
          "id": "1",
          "description": "NetworkPolicy deny-all exists in development",
          "verificationScriptFile": "q7_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "8",
      "namespace": "naboo",
      "machineHostname": "ckad9999",
      "question": "NetworkPolicy – Allow from Stage and QA\n\nCreate network policy that allow only traffic to all pods in namespace `naboo` from pods with label `environmental: stage` and from pods in namespace `qa`.\n\n```\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-from-stage-and-qa\n  namespace: naboo\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: qa\n    - podSelector:\n        matchLabels:\n          environmental: stage\n```",
      "concepts": ["NetworkPolicy", "podSelector", "namespaceSelector"],
      "verification": [
        {
          "id": "1",
          "description": "NetworkPolicy allow-from-stage-and-qa exists",
          "verificationScriptFile": "q8_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "9",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "Upgrade Kubernetes\n\nUpgrade a Kubernetes node. Drain the node, upgrade kubeadm and kubelet components, then uncordon the node.",
      "concepts": ["kubeadm", "upgrade", "kubelet", "drain", "uncordon"],
      "verification": [
        {
          "id": "1",
          "description": "Node upgraded successfully",
          "verificationScriptFile": "q9_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "10",
      "namespace": "team-coral",
      "machineHostname": "ckad9999",
      "question": "ServiceAccount Token Expiration\n\nPods should have annotation `token-lifetime` with value `1200`.\nServiceAccount `stream-multiplex` should be used.\nDisable automounting of ServiceAccount tokens.\nThe ServiceAccount token should be mounted at `/var/run/secrets/custom/` with an expiration of 1200s.\n\n```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: stream-multiplex\n  namespace: team-coral\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      id: stream-multiplex\n  template:\n    metadata:\n      labels:\n        id: stream-multiplex\n      annotations:\n        token-lifetime: \"1200\"\n    spec:\n      serviceAccountName: stream-multiplex\n      automountServiceAccountToken: false\n      containers:\n        - image: httpd:2-alpine\n          name: httpd\n          resources:\n            requests:\n              cpu: 20m\n              memory: 20Mi\n          volumeMounts:\n            - name: token-volume\n              mountPath: /var/run/secrets/custom\n              readOnly: true\n      volumes:\n        - name: token-volume\n          projected:\n            sources:\n              - serviceAccountToken:\n                  path: token\n                  expirationSeconds: 1200\n```",
      "concepts": ["ServiceAccount", "projected volume", "token expiration"],
      "verification": [
        {
          "id": "1",
          "description": "Deployment has token config and mount",
          "verificationScriptFile": "q10_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "11",
      "namespace": "code",
      "machineHostname": "ckad9999",
      "question": "TLS Secret – Create and Mount\n\nCreate a TLS secret named `code-secret` in namespace `code` using the certificate files located at `/root/custom-cert.crt` and `/root/custom-key.key`.\n\nThen edit the deployment `code-server` in namespace `code` to mount this secret as a volume at `/etc/code/tls` with read-only access.",
      "concepts": ["secrets", "TLS", "volume", "volumeMount"],
      "verification": [
        {
          "id": "1",
          "description": "TLS secret and volume mount configured",
          "verificationScriptFile": "q11_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "12",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "Docker Security\n\nRemove user from docker group and secure docker:\n\n- Change the ownership of `/var/run/docker.sock` to root:root\n- Add `--group=root` to the ExecStart of docker systemd file\n- Modify `/etc/docker/daemon.json` to remove TCP section, keep only unix socket\n- Restart docker service",
      "concepts": ["Docker", "daemon.json", "socket", "security"],
      "verification": [
        {
          "id": "1",
          "description": "Docker secured as required",
          "verificationScriptFile": "q12_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "13",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "Falco\n\nGet the pod that uses this folder `/dev/x`.\n\nAdd a Falco rule to detect access to files starting with `/dev/x` and scale the deployment that uses this folder to zero.",
      "concepts": ["Falco", "rules", "runtime security"],
      "verification": [
        {
          "id": "1",
          "description": "Falco rule and scaling applied",
          "verificationScriptFile": "q13_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "14",
      "namespace": "team-white",
      "machineHostname": "ckad9999",
      "question": "ImagePolicyWebhook – AdmissionConfiguration\n\nCreate an AdmissionConfiguration at `/opt/course/12/webhook/admission-config.yaml` with ImagePolicyWebhook configuration.\n\nConfigure the apiserver to:\n- Mount `/opt/course/12/webhook` at `/etc/kubernetes/webhook`\n- Use the AdmissionConfiguration at the mounted path\n- Enable the ImagePolicyWebhook admission plugin\n\nFix the admission config and kubeconfig YAML files as needed.",
      "concepts": ["ImagePolicyWebhook", "AdmissionConfiguration", "apiserver"],
      "verification": [
        {
          "id": "1",
          "description": "AdmissionConfiguration and apiserver configured",
          "verificationScriptFile": "q14_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "15",
      "namespace": "restricted",
      "machineHostname": "ckad9999",
      "question": "Pod Security Standard\n\nIn namespace `restricted` (enforcing restricted Pod Security), fix the deployment `web-server` so its pods can run.\n\nSet the securityContext to comply with restricted policy and remove `runAsUser: 0`.",
      "concepts": ["Pod Security Admission", "restricted", "securityContext"],
      "verification": [
        {
          "id": "1",
          "description": "Deployment complies with restricted PSS",
          "verificationScriptFile": "q15_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "16",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "Audit Log Policy\n\nAudit Logging has been enabled in the cluster with an Audit Policy located at `/etc/kubernetes/audit/policy.yaml`.\n\nChange the configuration so that only one backup of the logs is stored.\n\nAlter the Policy in a way that it only stores logs:\n- From Secret resources, level Metadata\n- From `system:nodes` userGroups, level RequestResponse\n- Catch all other metadata logs\n\nAfter you update the Policy make sure to empty the log file at `/etc/kubernetes/audit/logs/audit.log`.",
      "concepts": ["audit", "audit policy", "apiserver"],
      "verification": [
        {
          "id": "1",
          "description": "Audit policy and log configured",
          "verificationScriptFile": "q16_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    }
  ]
}
