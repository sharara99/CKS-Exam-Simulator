{
  "questions": [
    {
      "id": "1",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "CIS Benchmark – Kubelet\n\nApply CIS benchmark settings for kubelet:\n- In kubelet config: set anonymous auth to false (in config file), use Webhook for authorization mode.\n- In etcd manifest (`/etc/kubernetes/manifests/etcd.yaml`): add `--client-cert-auth=true` and `--peer-client-cert-auth=true`; remove `--insecure-listen-address`.\n\nUse `kubectl -n kube-system edit cm kubelet-config`, then `kubeadm upgrade node phase kubelet-config`, then `service kubelet restart`.",
      "concepts": ["CIS benchmark", "kubelet", "etcd", "security"],
      "verification": [
        {
          "id": "1",
          "description": "Kubelet and etcd configuration applied",
          "verificationScriptFile": "q1_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "2",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "Remove Anonymous Access to API Server\n\nConfigure the API server to:\n- `--anonymous-auth=false`\n- `--authorization-mode=NODE,RBAC`\n- `--enable-admission-plugins=NodeRestriction`\n\nThen remove the ClusterRoleBinding named `anonymous-access` that grants anonymous access to the cluster.",
      "concepts": ["apiserver", "RBAC", "anonymous-auth", "NodeRestriction"],
      "verification": [
        {
          "id": "1",
          "description": "API server and ClusterRoleBinding updated",
          "verificationScriptFile": "q2_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "3",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "Image Policy Webhook\n\nEnable ImagePolicyWebhook in the API server using a config file:\n1. Update the config file (it's in JSON format) to implicit deny by setting parameter `defaultAllow` to `false` instead of `true`.\n2. Update kubeconfig file to point to webhook server URL shared in the question.\n3. Update kube-apiserver manifest to enable image policy plugin:\n   `--enable-admission-plugins=ImagePolicyWebhook`\n   `--admission-control-config-file=<path of config file>`",
      "concepts": ["ImagePolicyWebhook", "admission control", "apiserver"],
      "verification": [
        {
          "id": "1",
          "description": "ImagePolicyWebhook configured",
          "verificationScriptFile": "q3_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "4",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "Deployment – readOnlyRootFilesystem\n\nUpdate deployments to be `readOnlyRootFilesystem` to `true` to be immutable.",
      "concepts": ["deployments", "securityContext", "readOnlyRootFilesystem"],
      "verification": [
        {
          "id": "1",
          "description": "Deployments have readOnlyRootFilesystem true",
          "verificationScriptFile": "q4_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "5",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "Deployment – Security Context (2 containers)\n\nUpdate deployment that has 2 containers with security context in each pod with the below:\n- `runAsUser: 30000`\n- `allowPrivilegeEscalation: false`\n- `readOnlyRootFilesystem: true`",
      "concepts": ["deployments", "securityContext", "runAsUser", "allowPrivilegeEscalation"],
      "verification": [
        {
          "id": "1",
          "description": "Deployment has required security context",
          "verificationScriptFile": "q5_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "6",
      "namespace": "team-sedum",
      "machineHostname": "ckad9999",
      "question": "Istio Security and mTLS\n\nIstio has been installed in the cluster. Enable Istio sidecar injection for the whole Namespace and ensure all current and future Pods are running with the Istio proxy sidecar.\n\nUse: `kubectl get ns --show-labels`, `kubectl label ns team-sedum istio-injection=enabled`, then `kubectl -n team-sedum rollout restart deploy` for all deployments.",
      "concepts": ["Istio", "sidecar injection", "mTLS"],
      "verification": [
        {
          "id": "1",
          "description": "Namespace has istio-injection and pods have sidecar",
          "verificationScriptFile": "q6_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "7",
      "namespace": "development",
      "machineHostname": "ckad9999",
      "question": "NetworkPolicy – Deny All Traffic\n\nCreate network policy that denies all egress and ingress traffic to pods in `development` namespace.\n\nUse:\n```\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: deny-all-traffic\n  namespace: development\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n```",
      "concepts": ["NetworkPolicy", "ingress", "egress", "deny-all"],
      "verification": [
        {
          "id": "1",
          "description": "NetworkPolicy deny-all exists in development",
          "verificationScriptFile": "q7_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "8",
      "namespace": "naboo",
      "machineHostname": "ckad9999",
      "question": "NetworkPolicy – Allow from Stage and QA\n\nCreate network policy that allow only traffic to all pods in namespace `naboo` from pods with label `environmental: stage` and from pods in namespace `qa`.\n\nUse:\n```\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-from-stage-and-qa\n  namespace: naboo\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - namespaceSelector:\n        matchLabels:\n          name: qa\n    - podSelector:\n        matchLabels:\n          environmental: stage\n```",
      "concepts": ["NetworkPolicy", "podSelector", "namespaceSelector"],
      "verification": [
        {
          "id": "1",
          "description": "NetworkPolicy allow-from-stage-and-qa exists",
          "verificationScriptFile": "q8_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "9",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "Upgrade Kubernetes\n\nUpgrade a Kubernetes node:\n- `kubectl drain <node-name> --ignore-daemonsets`\n- SSH to the node\n- `apt update`\n- `apt-mark hold kubeadm`\n- `kubeadm upgrade node`\n- `apt-mark unhold kubectl kubelet`\n- `apt install kubelet=<version> kubectl=<version>`\n- `service kubelet restart`\n- `apt-mark hold kubelet kubectl`\n- `kubectl uncordon <node-name>`",
      "concepts": ["kubeadm", "upgrade", "kubelet", "drain", "uncordon"],
      "verification": [
        {
          "id": "1",
          "description": "Node upgraded successfully",
          "verificationScriptFile": "q9_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "10",
      "namespace": "team-coral",
      "machineHostname": "ckad9999",
      "question": "ServiceAccount Token Expiration\n\nPods should have annotation `token-lifetime` with value `1200`.\nServiceAccount `stream-multiplex` should be used.\nDisable automounting of ServiceAccount tokens.\nThe ServiceAccount token should be mounted at `/var/run/secrets/custom/` with an expiration of 1200s.\n\nUse a projected volume with `serviceAccountToken.expirationSeconds: 1200`.",
      "concepts": ["ServiceAccount", "projected volume", "token expiration"],
      "verification": [
        {
          "id": "1",
          "description": "Deployment has token config and mount",
          "verificationScriptFile": "q10_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "11",
      "namespace": "code",
      "machineHostname": "ckad9999",
      "question": "TLS Secret – Create and Mount\n\nFirst create the TLS secret:\n`kubectl create secret tls code-secret --cert=/root/custom-cert.crt --key=/root/custom-key.key -n code`\n\nThen edit the deployment to mount the secret:\n`kubectl edit deployment code-server -n code`\n\nUnder the `spec.template.spec` section, add:\n```\nvolumes:\n  - name: secret-volume\n    secret:\n      secretName: code-secret\n```\n\nUnder the `spec.template.spec.containers` section, add the volume mount:\n```\nvolumeMounts:\n  - name: secret-volume\n    mountPath: /etc/code/tls\n    readOnly: true\n```",
      "concepts": ["secrets", "TLS", "volume", "volumeMount"],
      "verification": [
        {
          "id": "1",
          "description": "TLS secret and volume mount configured",
          "verificationScriptFile": "q11_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "12",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "Docker Security\n\nRemove user from docker group:\n`gpasswd -d <username> docker`\n`newgrp $(id -gn)`\n\nSecure docker:\n- Change the ownership of the docker file: `chown root:root /var/run/docker.sock`\n- Add `--group=root` to the ExecStart of docker systemd file:\n  `systemctl edit docker`\n  Add:\n  ```\n  [Service]\n  ExecStart=\n  ExecStart=/usr/bin/dockerd --group=root\n  ```\n- Reload docker daemon: `systemctl daemon-reload` and `systemctl restart docker`\n- Modify `/etc/docker/daemon.json` to remove TCP section, keep only:\n  ```\n  {\n    \"hosts\": [\"unix:///var/run/docker.sock\"]\n  }\n  ```\n- Restart docker again.",
      "concepts": ["Docker", "daemon.json", "socket", "security"],
      "verification": [
        {
          "id": "1",
          "description": "Docker secured as required",
          "verificationScriptFile": "q12_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "13",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "Falco\n\nGet the pod that uses this folder `/dev/x`.\n\nAdd a Falco rule:\n```\n- rule: Custom Rule 1\n  desc: Custom Rule 1\n  condition: container and fd.name startswith /dev/x\n  output: custom_rule_1 file=%fd.name container=%container.id\n  priority: WARNING\n```\n\nAnd scale this deployment to zero.",
      "concepts": ["Falco", "rules", "runtime security"],
      "verification": [
        {
          "id": "1",
          "description": "Falco rule and scaling applied",
          "verificationScriptFile": "q13_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "14",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "ImagePolicyWebhook – AdmissionConfiguration\n\nCreate an AdmissionConfiguration at `/opt/course/12/webhook/admission-config.yaml` which contains the following ImagePolicyWebhook configuration:\n```\nimagePolicy:\n  kubeConfigFile: /etc/kubernetes/webhook/webhook.yaml\n  allowTTL: 10\n  denyTTL: 10\n  retryBackoff: 20\n  defaultAllow: true\n```\n\nConfigure the apiserver to:\n- Mount `/opt/course/12/webhook` at `/etc/kubernetes/webhook`\n- Use the AdmissionConfiguration at path `/etc/kubernetes/webhook/admission-config.yaml`\n- Enable the ImagePolicyWebhook admission plugin:\n  `--enable-admission-plugins=ImagePolicyWebhook`\n  `--admission-control-config-file=/etc/kubernetes/webhook/admission-config.yaml`\n\nFix the admission config and kubeconfig YAML files as needed.",
      "concepts": ["ImagePolicyWebhook", "AdmissionConfiguration", "apiserver"],
      "verification": [
        {
          "id": "1",
          "description": "AdmissionConfiguration and apiserver configured",
          "verificationScriptFile": "q14_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "15",
      "namespace": "restricted",
      "machineHostname": "ckad9999",
      "question": "Pod Security Standard\n\nIn namespace `restricted` (enforcing restricted Pod Security), fix the deployment `web-server` so its pods can run.\n\nSet the securityContext:\n```\nsecurityContext:\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop:\n    - ALL\n  runAsNonRoot: true\n  seccompProfile:\n    type: RuntimeDefault\n```\n\nAlso, remove `runAsUser: 0`.\n\nUse: `kubectl edit deployment web-server -n restricted`",
      "concepts": ["Pod Security Admission", "restricted", "securityContext"],
      "verification": [
        {
          "id": "1",
          "description": "Deployment complies with restricted PSS",
          "verificationScriptFile": "q15_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    },
    {
      "id": "16",
      "namespace": "default",
      "machineHostname": "ckad9999",
      "question": "Audit Log Policy\n\nAudit Logging has been enabled in the cluster with an Audit Policy located at `/etc/kubernetes/audit/policy.yaml`.\n\nChange the configuration so that only one backup of the logs is stored.\n\nAlter the Policy in a way that it only stores logs:\n- From Secret resources, level Metadata\n- From `system:nodes` userGroups, level RequestResponse\n- Catch all other metadata logs\n\nAfter you update the Policy make sure to empty the log file so it only contains entries according to your changes, like using `echo > /etc/kubernetes/audit/logs/audit.log`.",
      "concepts": ["audit", "audit policy", "apiserver"],
      "verification": [
        {
          "id": "1",
          "description": "Audit policy and log configured",
          "verificationScriptFile": "q16_s1_validate.sh",
          "expectedOutput": "0",
          "weightage": 6
        }
      ]
    }
  ]
}
